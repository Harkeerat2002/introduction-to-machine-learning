{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor([0., 1., 2.])\n",
    "X_train = X_train.to(device)\n",
    "X_train.is_cuda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `generate_embeddings()`\n",
    "\n",
    "This method is responsible for transforming, resizing, and normalizing images using pre-trained ResNet50. It extracts embeddings from images using the same pre-trained model and saves the embeddings as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings():\n",
    "    \"\"\"\n",
    "    Transform, resize and normalize the images and then use a pretrained model to extract \n",
    "    the embeddings.\n",
    "    \"\"\"\n",
    "    train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=\"dataset/\", transform=train_transforms)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=64,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True, num_workers=16)\n",
    "\n",
    "    \n",
    "    model = resnet50(weights='DEFAULT', pretrained=True)\n",
    "    model = nn.Sequential(*list(model.children())[:-1])\n",
    "    model.to(device)\n",
    "\n",
    "    embeddings = []\n",
    "    embedding_size = 2048\n",
    "    num_images = len(train_dataset)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            feats = model(imgs)\n",
    "            embeddings.append(feats.cpu().numpy())\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    embeddings = embeddings.reshape(num_images, embedding_size)\n",
    "\n",
    "    np.save('dataset/embeddings.npy', embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ged_data()`\n",
    "\n",
    "This method loads triplets from a file and generates features and labels for the triplets. The labels are set to 1 for the triplet in the file and 0 for negative triplets generated for data augmentation. The method also applies normalization to the embeddings which was the task to be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file, train=True):\n",
    "    \"\"\"\n",
    "    Load the triplets from the file and generate the features and labels.\n",
    "\n",
    "    input: file: string, the path to the file containing the triplets\n",
    "          train: boolean, whether the data is for training or testing\n",
    "\n",
    "    output: X: numpy array, the features\n",
    "            y: numpy array, the labels\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line)\n",
    "\n",
    "    # generate training data from triplets\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\",\n",
    "                                         transform=None)\n",
    "    filenames = [s[0].split('/')[-1].replace('.jpg', '')\n",
    "                 for s in train_dataset.samples]\n",
    "    embeddings = np.load('dataset/embeddings.npy')\n",
    "\n",
    "    # Normalize the embeddings\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    file_to_embedding = {}\n",
    "    for i in range(len(filenames)):\n",
    "        file_to_embedding[filenames[i]] = embeddings[i]\n",
    "    X = []\n",
    "    y = []\n",
    "    # use the individual embeddings to generate the features and labels for triplets\n",
    "    for t in triplets:\n",
    "        emb = [file_to_embedding[a] for a in t.split()]\n",
    "        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n",
    "        y.append(1)\n",
    "        # Generating negative samples (data augmentation)\n",
    "        if train:\n",
    "            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n",
    "            y.append(0)\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader_from_np(X, y=None, train=True, batch_size=64, shuffle=True, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n",
    "\n",
    "    input: X: numpy array, the features\n",
    "           y: numpy array, the labels\n",
    "\n",
    "    output: loader: torch.data.util.DataLoader, the object containing the data\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float),\n",
    "                                torch.from_numpy(y).type(torch.long))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        pin_memory=True, num_workers=num_workers)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size=2048):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "\n",
    "        :param embedding_size: the size of the image embedding.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Creating new layers on top of the pretrained model\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(3 * embedding_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        :param x: torch.Tensor, the input to the model.\n",
    "\n",
    "        :return: torch.Tensor, the output of the model.\n",
    "        \"\"\"\n",
    "        x = self.fc_layers(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train_model()`\n",
    "\n",
    "This method trains the model using the features and labels generated by `get_data()` method. It uses the triplet loss function to train the model. The model is trained for 20 epochs and the weights are saved after every epoch. The models are trained on the CPU as th GPU setup on the local environment did not worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader):\n",
    "    \"\"\"\n",
    "    The training procedure of the model; it accepts the training data, defines the model \n",
    "    and then trains it.\n",
    "\n",
    "    input: train_loader: torch.data.util.DataLoader, the object containing the training data\n",
    "    \n",
    "    output: model: torch.nn.Module, the trained model\n",
    "    \"\"\"\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    number_of_epochs = 20\n",
    "\n",
    "    criterion = nn.BCELoss().to(device) # Loss Function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Optimizer\n",
    "\n",
    "    random_generator = torch.Generator(device=device)\n",
    "    train_data, valid_data = random_split(train_loader.dataset, [0.8, 0.2], generator=random_generator)\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(number_of_epochs):\n",
    "        epoch = epoch + 1\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output.squeeze(), target.type_as(output))\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output.squeeze(), target.type_as(output))\n",
    "                valid_loss += loss.item()\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        \n",
    "\n",
    "        # Taken from college\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader):\n",
    "    \"\"\"\n",
    "    The testing procedure of the model; it accepts the testing data and the trained model and \n",
    "    then tests the model on it.\n",
    "\n",
    "    input: model: torch.nn.Module, the trained model\n",
    "           loader: torch.data.util.DataLoader, the object containing the testing data\n",
    "\n",
    "    output: None, the function saves the predictions to a results.txt file\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad():  # We don't need to compute gradients for testing\n",
    "        for [x_batch] in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            predicted = model(x_batch)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            # Rounding the predictions to 0 or 1\n",
    "            predicted[predicted >= 0.5] = 1\n",
    "            predicted[predicted < 0.5] = 0\n",
    "            predictions.append(predicted)\n",
    "        predictions = np.vstack(predictions)\n",
    "    np.savetxt(\"results.txt\", predictions, fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function. You don't have to change this\n",
    "if __name__ == '__main__':\n",
    "    TRAIN_TRIPLETS = 'train_triplets.txt'\n",
    "    TEST_TRIPLETS = 'test_triplets.txt'\n",
    "\n",
    "    # generate embedding for each image in the dataset\n",
    "    if (os.path.exists('dataset/embeddings.npy') == False):\n",
    "        generate_embeddings()\n",
    "\n",
    "    # load the training and testing data\n",
    "    X, y = get_data(TRAIN_TRIPLETS)\n",
    "    X_test, _ = get_data(TEST_TRIPLETS, train=False)\n",
    "\n",
    "    print(\"Loaded data\")\n",
    "\n",
    "    # Create data loaders for the training and testing data\n",
    "    train_loader = create_loader_from_np(X, y, train=True, batch_size=64)\n",
    "    test_loader = create_loader_from_np(\n",
    "        X_test, train=False, batch_size=2048, shuffle=False)\n",
    "    \n",
    "    print(\"Created data loaders\")\n",
    "\n",
    "    # define a model and train it\n",
    "    model = train_model(train_loader)\n",
    "\n",
    "    print(\"Trained model\")\n",
    "\n",
    "    # test the model on the test data\n",
    "    test_model(model, test_loader)\n",
    "    print(\"Results saved to results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelor-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
